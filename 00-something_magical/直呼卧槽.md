## value based

### 策略迭代（policy iteration）

- state value  
  - '1': 66.65674655343062 
  - '2': 66.65674655343062
- Q value 
  - ('1', 'a'): 66.65684575456297 
  - ('1', 'b'): 66.32351242122964 
  - ('2', 'a'): 66.65684575456297 
  - ('2', 'b'): 66.32351242122964

### 值迭代（value iteration）

- state value
  -  '1': 66.65675893118923
  -  '2': 66.65675893118923
- Q value
  - ('1', 'a'): 66.65685800854399
  -  ('1', 'b'): 66.32352467521066,
  -  ('2', 'a'): 66.65685800854399
  -  ('2', 'b'): 66.32352467521066

### 对比策略迭代和值迭代

策略迭代和值迭代都可以得到相同的策略，同时 状态值v和动作值 q也都是完全相同的。  

 **在求解上可以视为 策略迭代法和值迭代法是等价的。**

这个两个算法的本质区别：数值分析中的explicit method和implicit method。

explicit收敛慢、稳定性差；implicit需要解方程

### Q learning

- '1','a': 68.40938741672667
-  '1','b': 66.34559929687401
-  '2','a': 68.5217228134981
-  '2','b': 66.71533983503035

### SARSA

- '1','a': 65.33289368868292
-  '1','b': 64.84874502557145
-  '2','a': 65.6544514744052
-  '2','b': 64.86533857319452

### 对比Q-learning和SARSA

**Q-Learning估计的结果确实普遍比真实值高，Sarsa方法估计的V值普遍比真实值低**





## value based和policy based 对比

基于值函数的方法（Q-learning, SARSA等等经典强化学习研究的大部分算法）存在策略退化问题，即值函数估计已经很准确了，但通过值函数得到的策略仍然不是最优

Policy Gradient不会出现策略退化现象，其目标表达更直接，求解方法更现代，还能够直接求解stochastic policy等等优点更加实用。

